{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Module to annotate TSV againts a Panel of Normals.\"\"\"\n",
    "import gzip\n",
    "from os.path import basename, dirname, join\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "LOGGER = logging.getLogger(\"annotvcf\")\n",
    "LOGGER_LEVEL = logging.DEBUG\n",
    "LOGGER_FORMAT = \"[%(levelname)s] - %(asctime)s: %(message)s\"\n",
    "LOGGER_DATEFMT = \"%m/%d/%Y %I:%M:%S %p\"\n",
    "LOGGER.setLevel(LOGGER_LEVEL)\n",
    "\n",
    "# Create console handler and set level to debug.\n",
    "HANDLER = logging.StreamHandler()\n",
    "HANDLER.setLevel(LOGGER_LEVEL)\n",
    "\n",
    "# Add formatter to handler.\n",
    "FORMATTER = logging.Formatter(LOGGER_FORMAT, datefmt=LOGGER_DATEFMT)\n",
    "HANDLER.setFormatter(FORMATTER)\n",
    "LOGGER.addHandler(HANDLER)\n",
    "LOGGER.propagate = False\n",
    "\n",
    "LOGGING_LEVELS = {\n",
    "    \"critical\": logging.CRITICAL,\n",
    "    \"error\": logging.ERROR,\n",
    "    \"warning\": logging.WARNING,\n",
    "    \"debug\": logging.DEBUG,\n",
    "    \"info\": logging.INFO,\n",
    "}\n",
    "\n",
    "def annot_normals(\n",
    "        input_file_name, normal_tsv, threshold, output_file_name, loglevel\n",
    "    ):\n",
    "    \"\"\"Annotate variants with internal normal controls.\"\"\"\n",
    "    LOGGER.setLevel(LOGGING_LEVELS.get(loglevel))\n",
    "    LOGGER.info(\"Running script for annotating against panel of normals...\")\n",
    "\n",
    "    # Create aggregated file for Pool of Panel of Normals\n",
    "    normals = __import_pool_normals(normal_tsv)\n",
    "    normals = __aggregate_pool_normals(normals)\n",
    "    outdir = dirname(output_file_name)\n",
    "    normal_tsv_name = basename(normal_tsv[0]).split('.tsv')[0]\n",
    "    normals_file = join(outdir, f\"{normal_tsv_name}.pon.normals.tsv.gz\")\n",
    "    nfile = gzip.open(normals_file, \"wt\")\n",
    "    normals.to_csv(\n",
    "        path_or_buf=nfile,\n",
    "        sep=\"\\t\",\n",
    "        float_format=\"%.3f\",\n",
    "        na_rep=\"NA\",\n",
    "    )\n",
    "    nfile.close()\n",
    "    LOGGER.info(\"Created pool of normals file: %s\", normals_file)\n",
    "    normals[\"CHR\"] = normals[\"CHR\"].astype(str)\n",
    "    normals = normals.set_index([\"CHR\", \"START\"]).sort_index()\n",
    "\n",
    "    # Get columns to annotate\n",
    "    common_columns = [\"Match_Class\", \"Counts\", \"Position\", \"Change\"]\n",
    "    extra_columns = list(set(normals.columns) - {'REF', 'ALT', 'COUNT'})\n",
    "    output_columns = [\n",
    "        f\"NORMALS_{field}\" for field in common_columns + extra_columns\n",
    "    ]\n",
    "\n",
    "    # Annotate Input File against Panel of Normals\n",
    "    if not output_file_name.endswith('.gz'):\n",
    "        output_file_name += \".gz\"\n",
    "\n",
    "    ifile = gzip.open(input_file_name, \"rt\")\n",
    "    ofile = gzip.open(output_file_name, \"wt\")\n",
    "\n",
    "    data = []\n",
    "    write_column_names = True\n",
    "    for line in ifile.readlines():\n",
    "        if line.startswith(\"#\"):\n",
    "            ofile.write(line)\n",
    "        elif line.startswith(\"ID_VARIANT\"):\n",
    "            input_columns = line.strip().split(\"\\t\")\n",
    "        else:\n",
    "            values = line.strip().split(\"\\t\")\n",
    "            data.append(values)\n",
    "\n",
    "        # Write Data by chunks of 10000 records\n",
    "        if len(data) >= 10000:\n",
    "            variants = pd.DataFrame(data=data, columns=input_columns)\n",
    "            variants[output_columns] = variants.apply(\n",
    "                lambda row: __annotate_line(\n",
    "                    row, normals, extra_columns, threshold\n",
    "                ),\n",
    "                axis=1\n",
    "            )\n",
    "            variants.to_csv(\n",
    "                path_or_buf=ofile,\n",
    "                header=write_column_names,\n",
    "                index=False,\n",
    "                sep=\"\\t\",\n",
    "                float_format=\"%.3f\",\n",
    "                na_rep=\"NA\",\n",
    "            )\n",
    "            data = []\n",
    "\n",
    "            # Flag to only write column names once in the file\n",
    "            if write_column_names:\n",
    "                write_column_names = False\n",
    "\n",
    "    # Write the last chunk of data when all lines are parsed\n",
    "    if data:\n",
    "        variants = pd.DataFrame(data=data, columns=input_columns)\n",
    "        variants[output_columns] = variants.apply(\n",
    "            lambda row: __annotate_line(\n",
    "                row, normals, extra_columns, threshold\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "        variants.to_csv(\n",
    "            path_or_buf=ofile,\n",
    "            header=write_column_names,\n",
    "            index=False,\n",
    "            sep=\"\\t\",\n",
    "            float_format=\"%.3f\",\n",
    "            na_rep=\"NA\",\n",
    "        )\n",
    "\n",
    "    ifile.close()\n",
    "    ofile.close()\n",
    "    LOGGER.info(\n",
    "        \"Created file with Pool of normals annotation: %s\", output_file_name\n",
    "    )\n",
    "\n",
    "\n",
    "def __annotate_line(line, normals, extra_columns, threshold):\n",
    "    \"\"\"\n",
    "    Annotate each variant with found variants in panel of normals.\n",
    "    A variant is annotated if there is a match, with 4 fixed columns, plus the\n",
    "    extra columns information regarding to VAF found in the normals file.\n",
    "    Fixed columns:\n",
    "        Match_class:    genomic_exact: exact match with variant.\n",
    "                        genomic_same_pos: other variants in same position.\n",
    "                        genomic_close_pos: other variants found close.\n",
    "        Counts:         Number of matches.\n",
    "        Positions:      START position.\n",
    "        Change:         REF > ALT.\n",
    "    Arguments:\n",
    "        line (pd.Series): Dataframe row containing a variant information.\n",
    "        normals (pd.Dataframe): Dataframe with panel of normals info.\n",
    "        extra_columns (list): extra columns of panel of normals to annotate.\n",
    "        threshold (int): number of base pairs to define the search region.\n",
    "    Return:\n",
    "        pd.Series: Annotation added to each variant row.\n",
    "    \"\"\"\n",
    "    match_class = \"\"\n",
    "    chrom = line[\"CHR\"]\n",
    "    start = int(line[\"START\"])\n",
    "\n",
    "    if (chrom, start) in normals.index:\n",
    "        # variants in same position.\n",
    "        same_pos = normals.loc[(chrom, start):(chrom, start)]\n",
    "        # variants that match exactly both position and change.\n",
    "        exact_match = same_pos[\n",
    "            (same_pos[\"REF\"] == line[\"REF\"]) &\n",
    "            (same_pos[\"ALT\"] == line[\"ALT\"])\n",
    "        ]\n",
    "        if not exact_match.empty:\n",
    "            match = exact_match\n",
    "            match_class = \"genomic_exact\"\n",
    "        else:\n",
    "            match = same_pos\n",
    "            match_class = \"genomic_same_pos\"\n",
    "    else:\n",
    "        # variants that are located close in matchition,\n",
    "        close_pos = normals.loc[\n",
    "            (chrom, start - threshold):(chrom, start + threshold)\n",
    "        ]\n",
    "        if not close_pos.empty:\n",
    "            match = close_pos\n",
    "            match_class = \"genomic_close_pos\"\n",
    "\n",
    "    if match_class:\n",
    "        annotation = [\n",
    "            match_class,\n",
    "            \", \".join(match[\"COUNT\"].astype(str)),\n",
    "            \", \".join([\n",
    "                str(i) for i in match.index.get_level_values(\"START\")\n",
    "            ]),\n",
    "            \", \".join([\n",
    "                f\"{a}>{b}\" for a, b in zip(match[\"REF\"], match[\"ALT\"])\n",
    "            ])\n",
    "        ]\n",
    "        annotation += [\n",
    "            \", \".join(match[column].astype(str))\n",
    "            for column in extra_columns\n",
    "        ]\n",
    "    else:\n",
    "        annotation = (4 + len(extra_columns)) * [None]\n",
    "    return pd.Series(annotation)\n",
    "\n",
    "\n",
    "def __import_pool_normals(normals_tsv):\n",
    "    \"\"\"Read all tsv files and generate an appended dataframe by sample rows.\"\"\"\n",
    "    pool = []\n",
    "    for path in normals_tsv:\n",
    "        try:\n",
    "            normals_chunks = pd.read_csv(\n",
    "                filepath_or_buffer=path,\n",
    "                compression=\"gzip\",\n",
    "                chunksize=20000,\n",
    "                sep=\"\\t\",\n",
    "                comment=\"#\",\n",
    "                low_memory=False,\n",
    "                dtype={'CHR': str},\n",
    "            )\n",
    "            pool.extend(normals_chunks)\n",
    "        except NameError:\n",
    "            raise Exception(f\"Error when reading {path}. File must be bgzip.\")\n",
    "\n",
    "    normals = pd.concat(pool)\n",
    "    LOGGER.info(\n",
    "        \"Creating normals using %s file(s), containing %s variant calls.\",\n",
    "        len(normals_tsv), normals.shape[0]\n",
    "    )\n",
    "    return normals\n",
    "\n",
    "\n",
    "def __aggregate_pool_normals(normals):\n",
    "    \"\"\"Aggregate a tsv file with normals of several patients, one by row.\"\"\"\n",
    "    callers = [\"caveman_\", \"pindel_\", \"mutect_\", \"strelka_\", \"\"]\n",
    "    targets = {\n",
    "        (caller + \"TARGET_VAF\"): caller\n",
    "        for caller in callers\n",
    "        if caller + \"TARGET_VAF\" in normals.columns\n",
    "    }\n",
    "    normals = normals[[\"CHR\", \"START\", \"REF\", \"ALT\"] + list(targets)]\n",
    "    normals = normals.groupby([\"CHR\", \"START\", \"REF\", \"ALT\"])\n",
    "    pon_agg = normals['ALT'].size().to_frame(name='COUNT')\n",
    "\n",
    "    for target, caller in targets.items():\n",
    "        pon_agg[caller + 'MEDIAN_VAF'] = (\n",
    "            normals[target].quantile(q=0.5).round(3)\n",
    "        )\n",
    "    for target, caller in targets.items():\n",
    "        pon_agg[caller + 'VAF_Q25'] = normals[target].quantile(q=0.25).round(3)\n",
    "        pon_agg[caller + 'VAF_Q75'] = normals[target].quantile(q=0.75).round(3)\n",
    "\n",
    "    pon_agg = pon_agg.reset_index()\n",
    "\n",
    "    LOGGER.info(\n",
    "        \"Aggregated normals containing %s unique variants.\", pon_agg.shape[0]\n",
    "    )\n",
    "    return pon_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] - 08/06/2019 06:23:54 PM: Creating normals using 1 file(s), containing 183425 variant calls.\n",
      "[INFO] - 08/06/2019 06:23:54 PM: Creating normals using 1 file(s), containing 183425 variant calls.\n"
     ]
    }
   ],
   "source": [
    "def __import_pool(tsvs):\n",
    "    \"\"\"Read all tsv files and generate an appended dataframe by sample rows.\"\"\"\n",
    "    for path in tsvs:\n",
    "        try:\n",
    "            indels = []\n",
    "            chunks = pd.read_csv(\n",
    "                filepath_or_buffer=path,\n",
    "                compression=\"gzip\",\n",
    "                chunksize=20000,\n",
    "                sep=\"\\t\",\n",
    "                comment=\"#\",\n",
    "                low_memory=False,\n",
    "                dtype={'CHR': str},\n",
    "            )\n",
    "            for i in chunks:\n",
    "                indels.append(i[i[\"ANY2_LCC\"].astype(bool)])\n",
    "        except NameError:\n",
    "            raise Exception(f\"Error when reading {path}. File must be bgzip.\")\n",
    "\n",
    "    pool = pd.concat(indels)\n",
    "    LOGGER.info(\n",
    "        \"Creating normals using %s file(s), containing %s variant calls.\",\n",
    "        len(tsvs), pool.shape[0]\n",
    "    )\n",
    "    return pool\n",
    "\n",
    "WGS_PASS_TSV = \"/work/isabl/data/analyses/30/02/173002/annot_indels__2.0.0__32.wg.pass_tsv.gz\"\n",
    "# WGS_PASS_TSV = \"/work/isabl/data/analyses/43/24/164324/pass/I-H-135076-T1-1-D1-1_vs_I-H-135076-N1-1-D1-1.indels.output.annot.tsv.gz\"\n",
    "\n",
    "pool = __import_pool([WGS_PASS_TSV])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pool)\n",
    "pool = __aggregate_pool_normals(pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(\"any2lcc_wgs.tsv.gz\", \"wt\") as nfile:\n",
    "    pool.to_csv(\n",
    "        path_or_buf=nfile,\n",
    "        sep=\"\\t\",\n",
    "        float_format=\"%.3f\",\n",
    "        na_rep=\"NA\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('indels_pass_p116.txt', 'r') as ifile:\n",
    "#     for pass_pattern in ifile.readlines()[:2]:\n",
    "#         pass_path = glob(pass_pattern.strip())[0]\n",
    "#         filename = basename(pass_path).split('.pass.vcf.gz')[0]\n",
    "        \n",
    "#         print(f'📖Reading file {pass_path}')\n",
    "#         indels = []\n",
    "#         chunks = pd.read_csv(\n",
    "#             filepath_or_buffer=pass_path,\n",
    "#             compression=\"gzip\",\n",
    "#             chunksize=20000,\n",
    "#             sep=\"\\t\",\n",
    "#             comment=\"#\",\n",
    "#             low_memory=False,\n",
    "#             dtype={'CHR': str},\n",
    "#         )\n",
    "#         for i in chunks:\n",
    "#             indels.append(i[i[\"ANY2_LCC\"].astype(bool)])\n",
    "        \n",
    "#         out_file_name = join('P116', f'{filename}.any2lcc.vcf')\n",
    "#         print(f'📖Saving file {pass_path}')\n",
    "#         with gzip.open(\"any2lcc_wgs.tsv.gz\", \"wt\") as ofile:\n",
    "#             pool.to_csv(\n",
    "#                 path_or_buf=ofile,\n",
    "#                 sep=\"\\t\",\n",
    "#                 float_format=\"%.3f\",\n",
    "#                 na_rep=\"NA\",\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
